---
title: "02_training_model"
author: "Miguel Ángel Armengol & Jay Chandra"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  #html_notebook:
    code_folding: hide
    number_sections: yes
    theme: flatly
    toc: yes
    toc_float: yes

knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = paste0(substr(inputFile,1,nchar(inputFile)-4)," ",Sys.Date(),'.html')) })
---


# Environment

```{r message=FALSE, warning=FALSE}
library(tidymodels)
library(xgboost)
library(ranger)
library(glmnet)
library(dplyr)
library(boot)
library(purrr)
```

# Subset de variables del modelo original

```{r}
df <- lactate_df_selected %>%
  select(actualicumortality, age_fixed, sex, unitType, hospitalAdmitSource,
         apache_iv, final_charlson_score, lactate_max,
         apache_strat, sofatotal_day1, teachingstatus, region) %>%
  mutate(actualicumortality = as.factor(actualicumortality)) %>%
  mutate(across(where(is.character), as.factor))
```


```{r}
library(tidymodels)
library(ggplot2)
library(boot)

set.seed(123)

# 1. Split de datos
data_split <- initial_split(df, prop = 0.8, strata = actualicumortality)
train_data <- training(data_split)
test_data  <- testing(data_split)

# 2. Receta
receta <- recipe(actualicumortality ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

# 3. Modelo xgboost
modelo <- boost_tree(
  trees = 1000,
  tree_depth = 6,
  learn_rate = 0.01
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# 4. Workflow y entrenamiento
flujo <- workflow() %>%
  add_model(modelo) %>%
  add_recipe(receta)

modelo_final <- fit(flujo, data = train_data)

# 5. Predicción de probabilidades
predicciones_prob <- predict(modelo_final, new_data = test_data, type = "prob") %>%
  bind_cols(test_data %>% select(actualicumortality))

# 6. Búsqueda del mejor umbral basado en F1
umbrales <- seq(0.05, 0.95, by = 0.01)

calcula_f1 <- function(umbral) {
  pred_temp <- predicciones_prob %>%
    mutate(
      actualicumortality = factor(actualicumortality, levels = levels(test_data$actualicumortality)),
      .pred_class = factor(if_else(.pred_1 >= umbral, 1, 0),
                           levels = levels(test_data$actualicumortality))
    )
  f_meas_vec(truth = pred_temp$actualicumortality, estimate = pred_temp$.pred_class)
}

f1_scores <- map_dbl(umbrales, calcula_f1)
umbral_optimo <- umbrales[which.max(f1_scores)]
cat("Umbral óptimo para F1:", umbral_optimo, "\n")

# 7. Predicciones finales con umbral óptimo
pred_final <- predicciones_prob %>%
  mutate(
    actualicumortality = factor(actualicumortality, levels = levels(test_data$actualicumortality)),
    .pred_class = factor(if_else(.pred_1 >= umbral_optimo, 1, 0),
                         levels = levels(test_data$actualicumortality))
  )

# 8. Métricas con umbral óptimo
metricas_optimas <- bind_rows(
  accuracy(pred_final, truth = actualicumortality, estimate = .pred_class),
  precision(pred_final, truth = actualicumortality, estimate = .pred_class),
  recall(pred_final, truth = actualicumortality, estimate = .pred_class),
  f_meas(pred_final, truth = actualicumortality, estimate = .pred_class)
)
print(metricas_optimas)

# 9. Matriz de confusión
autoplot(conf_mat(pred_final, truth = actualicumortality, estimate = .pred_class), type = "heatmap") +
  scale_fill_gradient(low = "white", high = "red") +
  theme_minimal() +
  labs(title = paste("Matriz de Confusión (XGBoost, umbral =", umbral_optimo, ")"), fill = "Count")

# 10. Bootstrap para IC del 95% con umbral óptimo
bootstrap_metric <- function(data, indices, metric_func, threshold) {
  d <- as_tibble(data[indices, ])
  d <- d %>%
    mutate(actualicumortality = factor(actualicumortality, levels = levels(test_data$actualicumortality)))

  pred <- predict(modelo_final, new_data = d, type = "prob") %>%
    bind_cols(d %>% select(actualicumortality)) %>%
    mutate(.pred_class = factor(if_else(.pred_1 >= threshold, 1, 0),
                                levels = levels(d$actualicumortality)))

  metric_func(truth = pred$actualicumortality, estimate = pred$.pred_class)
}

set.seed(123)
n_boot <- 100
boot_f1 <- boot(data = test_data, statistic = function(d, i) bootstrap_metric(d, i, f_meas_vec, umbral_optimo), R = n_boot)
boot_precision <- boot(data = test_data, statistic = function(d, i) bootstrap_metric(d, i, precision_vec, umbral_optimo), R = n_boot)
boot_recall <- boot(data = test_data, statistic = function(d, i) bootstrap_metric(d, i, recall_vec, umbral_optimo), R = n_boot)

ic_f1 <- boot.ci(boot_f1, type = "perc")$percent[4:5]
ic_precision <- boot.ci(boot_precision, type = "perc")$percent[4:5]
ic_recall <- boot.ci(boot_recall, type = "perc")$percent[4:5]

ic_df <- tibble(
  metric = c("F1", "Precision", "Recall"),
  IC_95_Lower = c(ic_f1[1], ic_precision[1], ic_recall[1]),
  IC_95_Upper = c(ic_f1[2], ic_precision[2], ic_recall[2])
)
print(ic_df)
```

# Explain

```{r}
library(xgboost)
library(dplyr)
library(tidymodels)

# 1. Extract the trained xgboost model from the workflow
xgb_model <- extract_fit_engine(modelo_final)

# 2. Prepare test data using the trained recipe
recipe_prepped <- prep(receta)
test_baked <- bake(recipe_prepped, new_data = test_data)

# 3. Check for non-numeric columns (should not exist if recipe worked)
non_numeric <- test_baked %>% select(where(~!is.numeric(.)))
if (ncol(non_numeric) > 0) {
  warning("The following columns are not numeric and will be removed:\n",
          paste(names(non_numeric), collapse = ", "))
}

# 4. Keep only numeric columns and convert to matrix
test_matrix <- test_baked %>%
  select(where(is.numeric)) %>%
  as.matrix()

# 5. Compute SHAP values
shap_values <- predict(xgb_model, newdata = test_matrix, predcontrib = TRUE)

# 6. Convert SHAP matrix to a tibble
shap_df <- as_tibble(shap_values)

# 7. Show the first rows
print(head(shap_df))

```


